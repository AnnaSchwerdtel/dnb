#+TITLE:
#+AUTHOR:
#+EMAIL:
#+KEYWORDS:
#+DESCRIPTION:
#+TAGS:
#+LANGUAGE: en
#+OPTIONS: toc:nil ':t H:5
#+STARTUP: hidestars overview
#+LaTeX_CLASS: scrartcl
#+LaTeX_CLASS_OPTIONS: [a4paper,11pt]
#+PANDOC_OPTIONS:

The goal is to enrich the DNB dataset and index it with Elastic.

* process DNB data
** download dump
#+BEGIN_SRC sh :results silent
  curl --output DNBTitel.rdf.gz "http://datendienst.dnb.de/cgi-bin/mabit.pl?cmd=fetch&userID=opendata&pass=opendata&mabheft=DNBTitel.rdf.gz"
#+END_SRC

** convert RDF to JSON
#+BEGIN_SRC sh
  rdf2json.py DNBTitel.rdf.gz | gzip -c > DNBTitel.json.gz
#+END_SRC

** TODO normalise fields
#+BEGIN_SRC sh
  ./json2json.py --normalise DNBTitel.json.gz | gzip -c > DNBTitel_normalised.json.gz
#+END_SRC

What is the difference between ~pages~ and ~extent~?
- ~rdf2json.py~: ~extent~ is from ~dcterms:extent~ and ~pages~ is from
  ~isbd:P1053~ (which is "has extent") - so it is basically the same
- We use ~extent~ when no valid ~pages~ are given.

* analyse fields
We analyse the distribution of some metadata fields.
** pages
*** dump the data
#+BEGIN_SRC sh
  ./json2json.py -p pages DNBTitel.json.gz > pages.tsv
#+END_SRC

Normalise and extract entries with meaningful (numerical) information:
#+BEGIN_SRC sh :results silent
  ./json2json.py --normalise --print "pages_norm" DNBTitel.json.gz \
      | grep "^[0-9][0-9]*$" | sort -S1G | uniq -c \
      | awk '{print $2"\t"$1}' | sort -n > pages_freq.tsv
#+END_SRC

*** plot the distribution
#+BEGIN_SRC gnuplot :results silent
reset
set term svg enhanced size 800,600
set out 'pages.svg'
set grid
set xrange [0:4000]
set logscale y
set format y "10^%T"

set xlabel 'number of pages'
set ylabel 'frequency'

plot 'pages_freq.tsv' using 1:2 with lines title ''

set term png enhanced size 800,600
set out 'pages.png'
replot

# zoom into range 400 to 600 to see 16-patterns of pages
set out 'pages_400-600.png'
unset logscale
unset format y
set xtics 0,16
set xrange [400:600]
plot 'pages_freq.tsv' using 1:2 with lines title ''

#+END_SRC

**** page distribution
[[pages.png]]

**** page range 400 to 600
[[pages_400-600.png]]

** extent
#+BEGIN_SRC sh :results silent
  ./json2json.py --norm --print "pages_norm,extent" DNBTitel.json.gz | sed "s/[0-9]/0/g" | gzip -c > pages_extent.tsv
#+END_SRC

#+BEGIN_SRC sh :results raw
  echo "pages_norm\textent\tfrequency"
  zcat pages_extent.tsv.gz | sort -S1G | uniq -c | head -n20
#+END_SRC

(after some manual tweaking:)

| pages_norm | extent | frequency |
|------------+--------+-----------|
|            |        |   5631051 |
|        000 |        |   4332641 |
|         00 |        |   2895689 |
|            | 00 S.  |   1020134 |
|          0 |        |    172825 |
|       0000 |        |     33250 |
|            | 000 S. |     16541 |
|            | 0 S.   |        95 |
|      00000 |        |        52 |
|     000000 |        |        31 |

So ~extent~ is only given when no (normalised) ~pages~ are given
-> set ~pages_norm~ to pages extracted from ~extent~ in those cases.

** issued
Extract and normalise the patterns for the "issued" field:
#+BEGIN_SRC sh :results silent
  ./json2json.py --norm --print "type,issued" DNBTitel.json.gz | sed "s/[0-9]/0/g" > issued.tsv
#+END_SRC

Let's have a look at the most frequent patterns:
#+BEGIN_SRC sh
  echo "type\tissued pattern\tfrequency"
  sort -S1G issued.tsv | uniq -c | sed -r 's/([0-9]) /\1\t/' | awk -F'\t' '{print $2"\t"$3"\t"$1}' | sort -t$'\t' -nrk3 | head -n20
  echo "*distinct pairs\t\t*" $(sort -S1G -u issued.tsv| wc -l)
#+END_SRC

| type             | issued pattern | frequency |
|------------------+----------------+-----------|
| Document         |           0000 |  10559276 |
| Issue            |           0000 |   1470687 |
| Article          |           0000 |    981040 |
| Collection       |                |    381824 |
| Periodical       |      0000-0000 |    304933 |
| Periodical       |          0000- |    155810 |
| Series           |          0000- |     62002 |
| Series           |              - |     42707 |
| Document         |                |     41579 |
| Periodical       |              - |     25939 |
| Document         |           00XX |     24172 |
| Series           |      0000-0000 |     18156 |
|                  |           0000 |     11070 |
| Collection       |           0000 |      9181 |
| Document         |      0000-0000 |      7250 |
| Periodical       |                |      2849 |
| Collection       |      0000-0000 |      2379 |
| Periodical       |           0000 |       443 |
| Article          |        0000/00 |       331 |
| Article          |          /0000 |       138 |
|------------------+----------------+-----------|
| *distinct pairs* |                |       105 |


Get the valid years for the "Document" type:
#+BEGIN_SRC sh :results silent
  ./json2json.py --normalise --print "type,issued" DNBTitel.json.gz \
      | grep -E '^Document\s+[0-9][0-9][0-9][0-9]$' \
      | awk -F'\t' '{print $2}' | sort | uniq -c | awk '{print $2"\t"$1}' \
						       > issued_document_distrib.tsv
#+END_SRC

Let's plot the years for the "Document" type:
#+BEGIN_SRC gnuplot :results silent
set term svg enhanced size 800,600
set out 'issued.svg'
set grid
set xrange [1450:2050]
set logscale y
# set format y "10^%T"

set xlabel 'year'
set ylabel 'frequency'

plot "issued_document_distrib.tsv" using 1:2 with lines title ''

set term png enhanced size 800,600
set out 'issued.png'
replot
#+END_SRC

[[issued.png]]

** medium

#+BEGIN_SRC sh
  ./json2json.py -n -p medium DNBTitel.json.gz | sort -S1G | uniq -c
#+END_SRC

| medium                                                  |   count |
|---------------------------------------------------------+---------|
|                                                         |  294526 |
| http://iflastandards.info/ns/isbd/terms/mediatype/T1008 |   19783 |
| RDACarrierType/1018                                     | 4001290 |
| RDACarrierType/1044                                     | 9604425 |
| RDAMediaType/1002                                       |   23059 |
| RDAMediaType/1003                                       |  159226 |

** place

#+BEGIN_SRC sh
  ./json2json.py -n -p place DNBTitel.json.gz | sort -S1G | uniq -c > place.tsv
#+END_SRC

#+BEGIN_SRC sh
  head place.tsv
#+END_SRC

#+RESULTS:
| 5106754 |             |          |                    |      |
|       1 | ['010']     |          |                    |      |
|       1 | ['0rleans'] |          |                    |      |
|       1 | ['1']       |          |                    |      |
|       1 | ['1010      | Wien,    | Blutgasse          | 3']  |
|       1 | ['1010      | Wien,    | Schubertring       | 3']  |
|       3 | ['10179     | Berlin'] |                    |      |
|       1 | ['1037      | Wien,    | Daffingerstraße    | 1']  |
|       1 | ['1050      | Wien,    | Kettenbrückengasse | 3']  |
|       1 | ['1070      | Wien,    | Lindengasse        | 47'] |

** price
** publisher

#+BEGIN_SRC sh
  ./json2json.py -n -p publisher DNBTitel.json.gz | sort -S1G | uniq -c > publisher.tsv
#+END_SRC

** contributor

#+BEGIN_SRC sh
  ./json2json.py -n -p contributor DNBTitel.json.gz | sort -S1G | uniq -c > contributor.tsv
#+END_SRC

* DONE enrich with Wikidata
By using the field ~creator~ (*or should we use ~contributor~?*).

** identify properties
For each entity in Wikidata that has a label, a GND id (P227)
property, and an occupation (P106) property, we extract the following
properties:

| id    | name                                | round | note                    |
|-------+-------------------------------------+-------+-------------------------|
| P106  | occupation                          |   1+2 | condition for inclusion |
| P227  | GND id                              |     1 | condition for inclusion |
| P21   | gender                              |     2 |                         |
| P569  | date of birth                       |     1 |                         |
| P19   | place of birth                      |     2 |                         |
| P625  | - coordinate location               |     2 | extract separately      |
| P570  | date of death                       |     1 |                         |
| P20   | place of death                      |     2 |                         |
| P625  | - coordinate location               |     2 | extract separately      |
| P103  | native language                     |     2 |                         |
| P1412 | languages spoken, written or signed |     2 |                         |
| P166  | awards received                     |     2 |                         |
| P18   | image (P18)                         |     1 |                         |

Approach:
1. find all entities with P106 and P227 and collect all other relevant
   properties
2. get the labels and missing values (e.g., coordinates of cities) for
   properties

** extract subclasses of writer
To label entities whose occupation property points to a subclass of
writer, we extract all subclasses of writer with SPARQL, since this is
faster and simpler than using the dump.

Since an entity can have several values for the occupation property
(e.g., [[https://www.wikidata.org/wiki/Q23][George Washington]]) we extract all values and if one of the
occupations is a subclass of writer, we label the entity as a writer.

We do this with curl as before:
#+BEGIN_SRC sparql :url https://query.wikidata.org/sparql :format text/csv
  SELECT ?subclass
  WHERE
  {
    ?subclass wdt:P279* wd:Q36180
  }
#+END_SRC

#+BEGIN_SRC sh :results silent
  curl \
      --header "Accept: text/tab-separated-values" \
      --output wikidata_writer_subclasses.tsv \
      --globoff \
       'https://query.wikidata.org/sparql?query=SELECT%20%3Fsubclass%20%3FsubclassLabel%0AWHERE%0A%7B%0A%20%20%3Fsubclass%20wdt%3AP279*%20wd%3AQ36180%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20...%20include%20the%20labels%0A%20%20%20%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%0A%20%20%7D%0A%7D'
#+END_SRC


#+BEGIN_SRC sh
  wc -l wikidata_writer_subclasses.tsv
#+END_SRC

#+RESULTS:
: 279 wikidata_writer_subclasses.tsv

** DONE process dump

Is done using Java (see ~WriterExtractor.java~ for the basic idea) and
 creates the file ~gndwriter.json~:

> Processed 32346937 entities in 2203 sec (14683 per second)
> read 357423 items and 69577 property values with missing labels


#+BEGIN_SRC sh :results raw
  grep "Goethe" gndwriter.json | sed -e "s/^,/{/" -e "s/$/}/" | json_pp 
#+END_SRC

#+BEGIN_SRC json
{
   "118540238" : {
      "id" : "Q5879",
      "name" : "Johann Wolfgang von Goethe",
      "occupations" : [
         {
            "id" : "Q4164507",
            "name" : "art critic"
         },
         {
            "id" : "Q3579035",
            "name" : "travel writer"
         },
         {
            "name" : "poet",
            "id" : "Q49757"
         },
         {
            "id" : "Q1209498",
            "name" : "poet lawyer"
         },
         {
            "name" : "music critic",
            "id" : "Q1350157"
         },
         {
            "name" : "novelist",
            "id" : "Q6625963"
         },
         {
            "name" : "autobiographer",
            "id" : "Q18814623"
         },
         {
            "name" : "playwright",
            "id" : "Q214917"
         },
         {
            "name" : "aphorist",
            "id" : "Q3606216"
         },
         {
            "id" : "Q18939491",
            "name" : "diarist"
         },
         {
            "id" : "Q1234713",
            "name" : "theologian"
         },
         {
            "name" : "art theorist",
            "id" : "Q17391638"
         }
      ]
   }
}
#+END_SRC

** DONE enrich JSON

Modifying ~json2json.py~ to add the Wikidata data for each found
writer with the ~--wikidata~ option.

#+BEGIN_SRC sh
  ./json2json.py -n -w gnditems_2017-08-22_15:03.json DNBTitel.json.gz \
      | gzip -c \
	     > DNBTitel_normalised_enriched.json.gz
#+END_SRC

** test enrichment

#+BEGIN_SRC 
  ./json2json.py -n -w gnditems_2017-08-22_15:03.json DNBTitel.json.gz | grep "poet lawyer" > poetlawyer_gndwriter.json
#+END_SRC

#+BEGIN_SRC sh :results raw
  grep Egmont poetlawyer_gndwriter.json | head -n1 | json_pp
#+END_SRC

#+BEGIN_SRC json
{
   "contributor" : [
      "116924373"
   ],
   "title" : "Goethes Egmont in Schillers Bearbeitung",
   "place_publisher" : "München ; Leipzig : G. Müller",
   "publisher" : "G. Müller",
   "place" : [
      "München",
      "Leipzig"
   ],
   "issued" : "1914",
   "lang" : "ger",
   "pages" : [
      "153 S."
   ],
   "medium" : "RDACarrierType/1044",
   "_id" : "361432887",
   "pages_norm" : 153,
   "creator_wd" : {
      "118540238" : {
         "languages" : "German",
         "image" : "Goethe (Stieler 1828).jpg",
         "place_of_death" : "Weimar",
         "native_language" : "German",
         "id" : "Q5879",
         "date_of_death" : "1832-03-22",
         "date_of_birth" : "1749-08-28",
         "name" : "Johann Wolfgang von Goethe",
         "awards" : [
            "Merit Order of the Bavarian Crown",
            "Officer of the Legion of Honour",
            "Order of Saint Anna, 1st class"
         ],
         "place_of_birth" : "Frankfurt",
         "gender" : "male",
         "occupation" : [
            "poet lawyer",
            "theatre manager",
            "botanist",
            "politician",
            "painter",
            "philosopher",
            "theologian",
            "jurist",
            "art critic",
            "music critic",
            "Geheimrat",
            "librarian",
            "poet",
            "travel writer",
            "physicist",
            "literary",
            "novelist",
            "playwright",
            "autobiographer",
            "diplomat",
            "statesman",
            "polymath",
            "aphorist",
            "diarist",
            "mineralogist",
            "zoologist",
            "art theorist",
            "lawyer"
         ],
         "occupation_writer" : [
            "poet lawyer",
            "theologian",
            "art critic",
            "music critic",
            "poet",
            "travel writer",
            "novelist",
            "playwright",
            "autobiographer",
            "aphorist",
            "diarist",
            "art theorist"
         ]
      }
   },
   "type" : "Document",
   "issued_norm" : 1914,
   "creator" : [
      "118540238"
   ]
}
#+END_SRC

** attic

Manually download (a part of) the Wikidata dump (since Java gets a 503
and disk space is scarce):
#+BEGIN_SRC sh
  # this fixes 
  zcat 20170814.json.gz_ORIG | head -n -2 | head -c -2 | sed -e "\$a]" | gzip -c > 20170814.json.gz 
#+END_SRC

* TODO index in Elastic

- check what happens with JSON like this: "publisher":
  "Akad. Kiado\u0301" - is the [[http://www.fileformat.info/info/unicode/char/0301/index.htm][COMBINING ACUTE ACCENT]] correctly
  processed? similar: "publisher": "Museum fu\u0308r Tierkunde"
Queries:
- Median, Mean, etc. in Elastic? - [[https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-aggregation.html][percentiles]]
- location (format "lat,lon" should work)

** TODO create index

TODO: add Wikidata fields

| field             | type    | analysed | note                                           |
|-------------------+---------+----------+------------------------------------------------|
| ~_id~             | string  | no       | DNB ID                                         |
| ~contributor~     | string  |          |                                                |
| ~creator~         | string  |          |                                                |
| ~extent~          | string  |          | field is missing! *TODO: difference to pages?* |
| ~issued~          | string  |          |                                                |
| ~issued_norm~     | integer | no       | year                                           |
| ~lang~            | string  | no       | 3-letter code or empty                         |
| ~medium~          | string  | no       |                                                |
| ~pages~           | string  | no       |                                                |
| ~pages_norm~      | integer | no       |                                                |
| ~place~           | string  |          |                                                |
| ~place_publisher~ | string  |          |                                                |
| ~price~           | string  |          |                                                |
| ~publisher~       | string  |          |                                                |
| ~short_title~     | string  |          |                                                |
| ~subject~         | string  |          |                                                |
| ~title~           | string  | yes      |                                                |
| ~type~            | string  | no       |                                                |

** fill index

* TODO analysis 
** TODO visualise pages
1. Sichtbarmachung des "extent"-Datenfeldes mit "HDT-it!" (ginge das?
hatte das damals auch für den Blogpost gemacht und finde, es schaut ganz
plastisch aus)

** DONE number of media, usable page data
2. Wieviele Medien insgesamt in der DNB – wie viele davon haben
brauchbare Seitenangaben usw. (eigentlich schon so, wie du es in
merging.org gemacht hast).

*** types of media
All media:
#+BEGIN_SRC sh
  ./json2json.py -n -p type DNBTitel.json.gz \
      | sort -S1G | uniq -c > media_freq.tsv
#+END_SRC

With usable page numbers:
#+BEGIN_SRC sh
  ./json2json.py -n -p type,pages_norm DNBTitel.json.gz \
      | awk -F'\t' '{if ($2 != "") print $1}' \
      | sort -S1G | uniq -c > media_with_pages_freq.tsv
#+END_SRC

#+BEGIN_SRC sh
  cat media_freq.tsv
  cat media_with_pages_freq.tsv.gz
#+END_SRC

| type       | frequency | frequency (proper pages given) |
|------------+-----------+--------------------------------|
|            |     11070 |                                |
| Article    |    981677 |                                |
| Collection |    393390 |                            347 |
| Document   |  10632628 |                        7434113 |
| Issue      |   1470688 |                        1036770 |
| Periodical |    489990 |                              8 |
| Series     |    122866 |                             20 |
|------------+-----------+--------------------------------|
| *sum*      |  14102309 |                        8471258 |
#+TBLFM: @9$2=vsum(@I..@II)::@9$3=vsum(@I..@II)

** DONE plot number of pages
3. Den Plot "number of pages", aber so, dass man sieht, dass es aller 16
Seiten Peaks gibt (hängt mit den Buchbögen zusammen, ein Bogen hat 16
Seiten, und die wollten die Setzer/Verlage dann eben auch füllen,
deswegen der Peak – der aber doch ziemlich interessant ist, weil er das
sichtbar macht, und unsere Datengrundlage irgendwie auch legitimiert).

see [[*plot the distribution][above]]

** TODO top authors
4. Ein paar weitere allgemeine Blicke in den Katalog: Autoren mit den
meisten Büchern im Katalog usw. (und die Beispiele aus merging.org,
Goethe und so).

- TODO: plot distribution of the number of authors per work 

*** by item count
#+BEGIN_SRC sh
  ./json2json.py -p "creator_wd.*.name" DNBTitel_normalised_enriched.json.gz | sort -S1G | uniq -c | sort -nr | head -n20
#+END_SRC

| author                     | frequency |
|----------------------------+-----------|
|                            |  12469257 |
| Johann Wolfgang von Goethe |      6764 |
| Rudolf Steiner             |      4320 |
| Hermann Hesse              |      3653 |
| Thomas Mann                |      3590 |
| Stefan Zweig               |      3262 |
| Heinz G. Konsalik          |      3251 |
| Wilhelm Grimm, Jacob Grimm |      3026 |
| Friedrich Schiller         |      2715 |
| William Shakespeare        |      2473 |
| Franz Kafka                |      2359 |
| Theodor Storm              |      2312 |
| Bertolt Brecht             |      2238 |
| Karl May                   |      2193 |
| Erich Kästner              |      2150 |
| Friedrich Nietzsche        |      2106 |
| Theodor Fontane            |      2030 |
| Wilhelm Busch              |      2014 |
| Gottfried Keller           |      2006 |
| Rainer Maria Rilke         |      1999 |

*** by page count

#+BEGIN_SRC sh :results silent
  ./json2json.py -p "creator_wd.*.name,pages_norm" DNBTitel_normalised_enriched.json.gz \
      | awk -F'\t' '{if ($2 != "") {sum[$1]+=$2; count[$1]+=1}} END {for (p in sum) printf("%s\t%s\t%s\t%s\n",  sum[p], count[p], int(sum[p]/count[p]), p)}' \
	    > author_pages_stats.tsv
#+END_SRC

#+BEGIN_SRC sh
  sort -nr author_pages_stats.tsv | head -n20
#+END_SRC

| author                     |      pages |   items | average pages |
|----------------------------+------------+---------+---------------|
|                            | 1051287200 | 7086618 |           148 |
| Johann Wolfgang von Goethe |    1201698 |    5221 |           230 |
| Heinz G. Konsalik          |     983426 |    3192 |           308 |
| Thomas Mann                |     931713 |    2517 |           370 |
| Hermann Hesse              |     738460 |    3339 |           221 |
| Rudolf Steiner             |     693045 |    3966 |           174 |
| Stefan Zweig               |     671318 |    2485 |           270 |
| Franz Kafka                |     538311 |    2017 |           266 |
| Fyodor Dostoyevsky         |     530589 |    1088 |           487 |
| Karl May                   |     507948 |    1504 |           337 |
| Friedrich Nietzsche        |     472979 |    1781 |           265 |
| Lion Feuchtwanger          |     459866 |    1023 |           449 |
| Theodor Fontane            |     453552 |    1562 |           290 |
| Marie Louise Fischer       |     435997 |    1677 |           259 |
| Colleen McCullough         |     424189 |     143 |          2966 |
| Erich Maria Remarque       |     414153 |    1179 |           351 |
| Leo Tolstoy                |     400056 |    1106 |           361 |
| Friedrich Schiller         |     399524 |    2077 |           192 |
| Heinrich Böll              |     388619 |    1769 |           219 |
| Stephen King               |     380219 |     799 |           475 |

*** by average page count

#+BEGIN_SRC sh
  sort -nrk3 author_pages_stats.tsv | head -n20
#+END_SRC

| author                              |  pages | items | average pages |
|-------------------------------------+--------+-------+---------------|
| Peter Götz von Olenhusen            | 190192 |     1 |        190192 |
| Antoine Furetière                   |  22924 |     2 |         11462 |
| Reinhard Baumgart                   | 340491 |    32 |         10640 |
| Karl Friedrich Masuhr               |  55128 |    10 |          5512 |
| César-Pierre Richelet               |  10588 |     2 |          5294 |
| Samael Aun Weor                     | 141110 |    27 |          5226 |
| Günther Bentele                     | 103739 |    27 |          3842 |
| Roland Berger                       |  27704 |     8 |          3463 |
| Jean Quatremer                      |   3376 |     1 |          3376 |
| Colleen McCullough                  | 424189 |   143 |          2966 |
| Michael Hoffmann-Becking            |   2811 |     1 |          2811 |
| Dieter Hildebrandt                  | 266207 |   100 |          2662 |
| André Vauchez                       |   2536 |     1 |          2536 |
| Peter Hartmann, Wolfgang Lauterbach |   2319 |     1 |          2319 |
| Wolfgang Kleiber                    |  10787 |     5 |          2157 |
| Noam Chomsky                        | 197964 |    92 |          2151 |
| Herbert Tröndle, Thomas Fischer     |   2052 |     1 |          2052 |
| Kari Jormakka                       |  10239 |     5 |          2047 |
| John Bernard Burke                  |   4052 |     2 |          2026 |
| Paul Robert                         |   4044 |     2 |          2022 |

There are probably some errors among those ...

#+BEGIN_SRC gnuplot :results silent
reset
set encoding utf8 
set term png enhanced size 800,600 font "sans-serif,11"
set out 'author_pages.png'

set grid
set datafile separator "\t"
set xrange [*:10000]
set logscale 
set format y "10^%T"
set format x "10^%T"

set xlabel 'number of items'
set ylabel 'mean number of pages per item'

#set arrow 1 from first 
set label "Peter Goetz von Olenhusen" left at 1, 190192 offset .5,.5
set label "Antoine Furetiere" left at 2, 11462 offset .5, .5
set label "Reinhard Baumgart" left at 32, 10640 offset .5, .5
set label "Colleen McCullough" left at 143, 2966 offset .5, .5
set label "Johann\nWolfgang\nvon\nGoethe" left at 5221, 230 offset -1.8, 3.6

plot 'author_pages_stats.tsv' using 2:3 with points pt 7 title ''

#+END_SRC

[[author_pages.png][author_pages.png]]

*** TODO merge author names from Wikidata using GND

** TODO publishers and pages
5. Hauptanliegen sollten für dieses Mal die Verlage und deren
Seitenpolitik sein: Durchschnittliche Länge von Büchern pro Verlag
(Suhrkamp, Rowohlt, Aufbau, Hanser, Eichborn, …) – wobei ich hier Bücher
über 5.000 Seiten weglassen würde, weil das offenbar Fehler sind. – Und
ein Längenranking (Top-20?) pro Verlag – die kann man dann
handbereinigen, falls mal nichtliterarische Werke darunter gefallen
sind, denn es sind ja nicht so viele in einer Top-20-Liste.

*Frage: wieviel Aufwand in die Normalisierung der Verlage stecken?*
Optionen: 
- nicht normalisieren
- einige wenige Verlage normalisieren

*** DONE most frequent publishers

#+BEGIN_SRC sh :results silent
  ./json2json.py -n -p publisher DNBTitel.json.gz | sort -S1G | uniq -c | gzip -c > publisher.tsv.gz
#+END_SRC

Top publishers:
#+BEGIN_SRC sh
  zcat publisher.tsv.gz | sort -S1G -nr | head -n20
#+END_SRC

| publisher                       |   items |
|---------------------------------+---------|
|                                 | 5640251 |
| GRIN Verlag GmbH                |  121456 |
| Books on Demand                 |   97716 |
| Springer                        |   83093 |
| LAP LAMBERT Academic Publishing |   83033 |
| [s. n.]                         |   78068 |
| Springer Berlin Heidelberg      |   54125 |
| Lang                            |   54075 |
| John Wiley & Sons               |   50768 |
| Heyne                           |   42233 |
| Rowohlt                         |   40982 |
| VDM Verlag Dr. Müller           |   40954 |
| tredition                       |   32839 |
| [s.n.]                          |   32549 |
| Herder                          |   31734 |
| GRIN Verlag                     |   31242 |
| Shaker                          |   29769 |
| Goldmann                        |   27502 |
| Beck                            |   27324 |
| Reclam                          |   26953 |

**** But: beware of errors
#+BEGIN_SRC sh
  zcat publisher.tsv.gz | sort -S1G -nr | grep Brockhaus | head -n20
#+END_SRC

| label                                          | frequency |
|------------------------------------------------+-----------|
| Brockhaus                                      |      5968 |
| R. Brockhaus                                   |       868 |
| F. A. Brockhaus                                |       671 |
| Brockhaus, VEB                                 |       543 |
| SCM R. Brockhaus                               |       494 |
| SCM R.Brockhaus im SCM-Verlag                  |       221 |
| VEB Brockhaus                                  |       193 |
| Bibliogr. Inst. und Brockhaus                  |       121 |
| [F. A. Brockhaus]                              |        65 |
| Brockhaus VEB                                  |        63 |
| Bibliogr. Inst. & Brockhaus                    |        62 |
| F. A. Brockhaus Verlag                         |        53 |
| Brockhaus, Wissenmedia in der InmediaONE] GmbH |        50 |
| SCM R. Brockhaus im SCM Verlag GmbH & Co.KG    |        38 |
| Theologischer Verlag Brockhaus                 |        34 |
| [Brockhaus]                                    |        30 |
| SCM R.Brockhaus                                |        25 |
| Theologischer Verl. Brockhaus                  |        16 |
| M. Brockhaus                                   |         9 |
| SCM Brockhaus                                  |         8 |
*** TODO average extent per publisher
**** extract raw data
#+BEGIN_SRC sh
  ./json2json.py -n -p publisher,pages_norm | gzip -c > publisher_pages.tsv.gz
#+END_SRC
**** DONE filter outliers and errors
remove:
- pages > 5000
- no pages
- no publisher

#+BEGIN_SRC sh
  zcat publisher_pages.tsv.gz \
      | awk -F'\t' '{if ($1 != "" && $2 != "" && $2 <= 5000) print $1"\t"$2}' \
      | gzip -c > publisher_pages_filtered.tsv.gz
#+END_SRC

**** TODO ranking per publisher

have to clarify normalisation first


**** DONE average book length per publisher

# would be easy with sqlite3 - install!

Count per publisher:
#+BEGIN_SRC sh :results silent
  zcat publisher_pages_filtered.tsv.gz \
      | awk -F'\t' '{sum[$1]+=$2; count[$1]+=1} END {for (p in sum) printf("%s\t%s\t%s\t%s\n", p, sum[p], count[p], int(sum[p]/count[p]))}' \
	    > publisher_pages_stats.tsv
#+END_SRC

***** top 20 by page sum
#+BEGIN_SRC sh
  sort -t$'\t' -rnk2 publisher_pages_stats.tsv | head -n20
#+END_SRC

| publisher                          |    pages | items | mean |
|------------------------------------+----------+-------+------|
| Springer                           | 21319843 | 65100 |  327 |
| Lang                               | 14134698 | 51255 |  276 |
| Heyne                              | 12587106 | 41094 |  306 |
| Beck                               | 10391848 | 24630 |  422 |
| Rowohlt                            |  9237092 | 40148 |  230 |
| Goldmann                           |  7776002 | 26491 |  294 |
| Herder                             |  5811171 | 29036 |  200 |
| Suhrkamp                           |  5675784 | 21129 |  269 |
| Ullstein                           |  5310460 | 19025 |  279 |
| Reclam                             |  4885858 | 25831 |  189 |
| Dt. Taschenbuch-Verl.              |  4489154 | 17122 |  262 |
| Piper                              |  4349318 | 14524 |  299 |
| Fischer-Taschenbuch-Verl.          |  4060245 | 14549 |  279 |
| Shaker                             |  4013108 | 22838 |  176 |
| RM-Buch-und-Medien-Vertrieb [u.a.] |  3813996 | 11215 |  340 |
| Weltbild                           |  3586937 | 10636 |  337 |
| Oldenbourg                         |  3469927 | 11820 |  294 |
| Thieme                             |  3317355 | 12559 |  264 |
| de Gruyter                         |  3281362 | 10534 |  312 |
| Kohlhammer                         |  3239885 | 14734 |  220 |

***** top 20 by mean page count

#+BEGIN_SRC sh
   sort -t$'\t' -rnk4 publisher_pages_stats.tsv | head -n20
#+END_SRC

| publisher                                                                    | pages | items | mean |
|------------------------------------------------------------------------------+-------+-------+------|
| Ronny Szpetecki                                                              |  4676 |     1 | 4676 |
| Kantonale Denkmalpflege Graubünden                                           |  4248 |     1 | 4248 |
| Großversandhaus Quelle                                                       |  3947 |     1 | 3947 |
| Didacta, Ausstellungs- und Verl.-Ges.                                        |  3700 |     1 | 3700 |
| Chemical Rubber Publishing Co.                                               |  3604 |     1 | 3604 |
| Deutscher Sparkassenverlag Stuttgart                                         |  3295 |     1 | 3295 |
| Ander                                                                        |  6398 |     2 | 3199 |
| Maṭbaʿat al-Ahrām                                                            |  3056 |     1 | 3056 |
| Deutsche Demokratische Republik, Staatl. Plankommission, Statist. Zentralamt |  2967 |     1 | 2967 |
| Burke's Peerage Ltd.                                                         |  2867 |     1 | 2867 |
| [PONS GmbH]                                                                  |  2837 |     1 | 2837 |
| Life Publ. International                                                     |  2776 |     1 | 2776 |
| Genfer Bibelgesellschaft                                                     |  2673 |     1 | 2673 |
| Hakubunkan Verl.                                                             |  2633 |     1 | 2633 |
| McClelland and Stewart Inc.                                                  |  2573 |     1 | 2573 |
| Schraad                                                                      |  2560 |     1 | 2560 |
| Verlagsh. Freya G. m. b. H.                                                  |  2516 |     1 | 2516 |
| Monte Avila                                                                  |  2516 |     1 | 2516 |
| Pierer, Heymann                                                              |  2500 |     1 | 2500 |
| Jixie-Gongye-Chubanshe                                                       |  2462 |     1 | 2462 |
***** scatter plot

How is the number of items per publisher related to the average number
of pages per publisher?

#+BEGIN_SRC gnuplot :results silent
reset
set term png enhanced size 800,600
set out 'publisher_pages.png'

set grid
set datafile separator "\t"
# set xrange [0:4000]
set logscale 
# set format y "10^%T"

set xlabel 'number of items
set ylabel 'mean number of pages per item'

plot 'publisher_pages_stats.tsv' using 3:4 with points pt 7 title ''
#+END_SRC

[[publisher_pages.png]]
