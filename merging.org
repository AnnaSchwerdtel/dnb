#+TITLE:
#+AUTHOR:
#+EMAIL:
#+KEYWORDS:
#+DESCRIPTION:
#+TAGS:
#+LANGUAGE: en
#+OPTIONS: toc:nil ':t H:5
#+STARTUP: hidestars overview
#+LaTeX_CLASS: scrartcl
#+LaTeX_CLASS_OPTIONS: [a4paper,11pt]
#+PANDOC_OPTIONS:

* download DNB dump
#+BEGIN_SRC sh :results silent
  curl --output DNBTitel.rdf.gz "http://datendienst.dnb.de/cgi-bin/mabit.pl?cmd=fetch&userID=opendata&pass=opendata&mabheft=DNBTitel.rdf.gz"
#+END_SRC

* convert RDF to JSON
#+BEGIN_SRC sh
  rdf2json.py DNBTitel.rdf.gz | gzip -c > DNBTitel.json.gz
#+END_SRC

* TODO normalise fields

- TODO :: difference between ~pages~ and ~extent~?

#+BEGIN_SRC sh
  ./json2json.py --normalise DNBTitel.json.gz | gzip -c > DNBTitel_normalised.json.gz
#+END_SRC

* analyse fields
We analyse the distribution of some metadata fields.
** pages
Dump the data:
#+BEGIN_SRC sh
  ./json2json.py -p pages DNBTitel.json.gz > pages.tsv
#+END_SRC

Normalise and extract entries with meaningful (numerical) information:
#+BEGIN_SRC sh :results silent
  ./json2json.py --normalise --print "pages_norm" DNBTitel.json.gz \
      | grep "^[0-9][0-9]*$" | sort -S1G | uniq -c \
      | awk '{print $2"\t"$1}' | sort -n > pages_freq.tsv
#+END_SRC

Plot the distribution:
#+BEGIN_SRC gnuplot :results silent
set term svg enhanced size 800,600
set out 'pages.svg'
set grid
set xrange [0:4000]
set logscale y
set format y "10^%T"

set xlabel 'number of pages'
set ylabel 'frequency'

plot 'pages_freq.tsv' using 1:2 with lines title ''

set term png enhanced size 800,600
set out 'pages.png'
replot
#+END_SRC

[[pages.png]]

** TODO extent
#+BEGIN_SRC sh :results silent
  ./json2json.py --norm --print "pages_norm,extent" DNBTitel.json.gz | sed "s/[0-9]/0/g" | gzip -c > pages_extent.tsv
#+END_SRC

#+BEGIN_SRC sh :results raw
  echo "pages_norm\textent\tfrequency"
  zcat pages_extent.tsv.gz | sort -S1G | uniq -c | head -n20
#+END_SRC

(after some manual tweaking:)

| pages_norm | extent | frequency |
|------------+--------+-----------|
|            |        |   5631051 |
|        000 |        |   4332641 |
|         00 |        |   2895689 |
|            | 00 S.  |   1020134 |
|          0 |        |    172825 |
|       0000 |        |     33250 |
|            | 000 S. |     16541 |
|            | 0 S.   |        95 |
|      00000 |        |        52 |
|     000000 |        |        31 |

*** TODO conclusion and task
So the extent is only given when no (normalised) pages are given
... -> set ~pages_norm~ to pages extracted from extent in those cases.

** issued
Extract and normalise the patterns for the "issued" field:
#+BEGIN_SRC sh :results silent
  ./json2json.py --norm --print "type,issued" DNBTitel.json.gz | sed "s/[0-9]/0/g" > issued.tsv
#+END_SRC

Let's have a look at the most frequent patterns:
#+BEGIN_SRC sh
  echo "type\tissued pattern\tfrequency"
  sort -S1G issued.tsv | uniq -c | sed -r 's/([0-9]) /\1\t/' | awk -F'\t' '{print $2"\t"$3"\t"$1}' | sort -t$'\t' -nrk3 | head -n20
  echo "*distinct pairs\t\t*" $(sort -S1G -u issued.tsv| wc -l)
#+END_SRC

| type             | issued pattern | frequency |
|------------------+----------------+-----------|
| Document         |           0000 |  10559276 |
| Issue            |           0000 |   1470687 |
| Article          |           0000 |    981040 |
| Collection       |                |    381824 |
| Periodical       |      0000-0000 |    304933 |
| Periodical       |          0000- |    155810 |
| Series           |          0000- |     62002 |
| Series           |              - |     42707 |
| Document         |                |     41579 |
| Periodical       |              - |     25939 |
| Document         |           00XX |     24172 |
| Series           |      0000-0000 |     18156 |
|                  |           0000 |     11070 |
| Collection       |           0000 |      9181 |
| Document         |      0000-0000 |      7250 |
| Periodical       |                |      2849 |
| Collection       |      0000-0000 |      2379 |
| Periodical       |           0000 |       443 |
| Article          |        0000/00 |       331 |
| Article          |          /0000 |       138 |
|------------------+----------------+-----------|
| *distinct pairs* |                |       105 |


Get the valid years for the "Document" type:
#+BEGIN_SRC sh :results silent
  ./json2json.py --normalise --print "type,issued" DNBTitel.json.gz \
      | grep -E '^Document\s+[0-9][0-9][0-9][0-9]$' \
      | awk -F'\t' '{print $2}' | sort | uniq -c | awk '{print $2"\t"$1}' \
                                                       > issued_document_distrib.tsv
#+END_SRC

Let's plot the years for the "Document" type:
#+BEGIN_SRC gnuplot :results silent
set term svg enhanced size 800,600
set out 'issued.svg'
set grid
set xrange [1450:2050]
set logscale y
# set format y "10^%T"

set xlabel 'year'
set ylabel 'frequency'

plot "issued_document_distrib.tsv" using 1:2 with lines title ''

set term png enhanced size 800,600
set out 'issued.png'
replot
#+END_SRC

[[issued.png]]

** medium

#+BEGIN_SRC sh
  ./json2json.py -n -p medium DNBTitel.json.gz | sort -S1G | uniq -c
#+END_SRC

| medium                                                  |   count |
|---------------------------------------------------------+---------|
|                                                         |  294526 |
| http://iflastandards.info/ns/isbd/terms/mediatype/T1008 |   19783 |
| RDACarrierType/1018                                     | 4001290 |
| RDACarrierType/1044                                     | 9604425 |
| RDAMediaType/1002                                       |   23059 |
| RDAMediaType/1003                                       |  159226 |

** place

#+BEGIN_SRC sh
  ./json2json.py -n -p place DNBTitel.json.gz | sort -S1G | uniq -c > place.tsv
#+END_SRC

#+BEGIN_SRC sh
  head place.tsv
#+END_SRC

#+RESULTS:
| 5106754 |             |          |                    |      |
|       1 | ['010']     |          |                    |      |
|       1 | ['0rleans'] |          |                    |      |
|       1 | ['1']       |          |                    |      |
|       1 | ['1010      | Wien,    | Blutgasse          | 3']  |
|       1 | ['1010      | Wien,    | Schubertring       | 3']  |
|       3 | ['10179     | Berlin'] |                    |      |
|       1 | ['1037      | Wien,    | Daffingerstraße    | 1']  |
|       1 | ['1050      | Wien,    | Kettenbrückengasse | 3']  |
|       1 | ['1070      | Wien,    | Lindengasse        | 47'] |

** price

** publisher

#+BEGIN_SRC sh
  ./json2json.py -n -p publisher DNBTitel.json.gz | sort -S1G | uniq -c > publisher.tsv
#+END_SRC

** contributor

#+BEGIN_SRC sh
  ./json2json.py -n -p contributor DNBTitel.json.gz | sort -S1G | uniq -c > contributor.tsv
#+END_SRC

* TODO enrich with Wikidata
By using the field ~creator~ (*or should we use ~contributor~?*).

** list of authors
For each entity in Wikidata that has a GND id (P227) and an occupation
(P106) property, we extract the following properties:

| id    | name                                | round | note                    |
|-------+-------------------------------------+-------+-------------------------|
| P106  | occupation                          | 1+2   | condition for inclusion |
| P227  | GND id                              | 1     | condition for inclusion |
| P21   | gender                              | 2?    |                         |
| P569  | date of birth                       | 1?    |                         |
| P19   | place of birth                      | 2     |                         |
| P625  | - coordinate location               | 2     |                         |
| P570  | date of death                       | 1?    |                         |
| P20   | place of death                      | 1?    |                         |
| P625  | - coordinate location               | 2     |                         |
| P103  | native language                     | 2     |                         |
| P1412 | languages spoken, written or signed | 2     |                         |
| P166  | awards received                     | 2     |                         |
| P18   | image (P18)                         | 1?    |                         |

Approach:
1. find all entities with P106 and P227 and collect all other relevant
   properties
2. for all properties, get the missing values (e.g., coordinates of
   cities)


[main] INFO org.wikidata.wdtk.dumpfiles.EntityTimerProcessor - Processed 32346937 entities in 2203 sec (14683 per second)
read 357423 items and 69577 property values with missing labels


*** all subclasses of writer
We do this with curl as before:
#+BEGIN_SRC sparql :url https://query.wikidata.org/sparql :format text/csv
  SELECT ?subclass
  WHERE
  {
    ?subclass wdt:P279* wd:Q36180
  }
#+END_SRC

#+BEGIN_SRC sh :results silent
  curl \
      --header "Accept: text/tab-separated-values" \
      --output wikidata_writer_subclasses.tsv \
      --globoff \
       'https://query.wikidata.org/sparql?query=SELECT%20%3Fsubclass%20%3FsubclassLabel%0AWHERE%0A%7B%0A%20%20%3Fsubclass%20wdt%3AP279*%20wd%3AQ36180%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20...%20include%20the%20labels%0A%20%20%20%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%0A%20%20%7D%0A%7D'
#+END_SRC


#+BEGIN_SRC sh
  wc -l wikidata_writer_subclasses.tsv
#+END_SRC

#+RESULTS:
: 279 wikidata_writer_subclasses.tsv

*** TODO writers

manually download (a part of) the Wikidata dump (since Java gets a 503
and disk space is scarce):
#+BEGIN_SRC sh
  # this fixes 
  zcat 20170814.json.gz_ORIG | head -n -2 | head -c -2 | sed -e "\$a]" | gzip -c > 20170814.json.gz 
#+END_SRC

*An entity can have several values for the occupation property - which
one to check? All? The first?* Example: [[https://www.wikidata.org/wiki/Q23][George Washington]]
And which one to output?

In the next round, consider this:
,----
| Another alternative would be to include the occupation for *all
| authors* and later on filter in Elastic ...
| 
| Thus: Keep it simple?!
| - add all occupations for all persons (with a GND id)
| - just add their names, ignore their ID (could we do this during indexing?)
`----


Then we extract all information about writers using
~WriterExtractor.java~ which creates the file ~gndwriter.json~:

#+BEGIN_SRC sh :results raw
  grep "Goethe" gndwriter.json | sed -e "s/^,/{/" -e "s/$/}/" | json_pp 
#+END_SRC


#+BEGIN_SRC json
{
   "118540238" : {
      "id" : "Q5879",
      "name" : "Johann Wolfgang von Goethe",
      "occupations" : [
         {
            "id" : "Q4164507",
            "name" : "art critic"
         },
         {
            "id" : "Q3579035",
            "name" : "travel writer"
         },
         {
            "name" : "poet",
            "id" : "Q49757"
         },
         {
            "id" : "Q1209498",
            "name" : "poet lawyer"
         },
         {
            "name" : "music critic",
            "id" : "Q1350157"
         },
         {
            "name" : "novelist",
            "id" : "Q6625963"
         },
         {
            "name" : "autobiographer",
            "id" : "Q18814623"
         },
         {
            "name" : "playwright",
            "id" : "Q214917"
         },
         {
            "name" : "aphorist",
            "id" : "Q3606216"
         },
         {
            "id" : "Q18939491",
            "name" : "diarist"
         },
         {
            "id" : "Q1234713",
            "name" : "theologian"
         },
         {
            "name" : "art theorist",
            "id" : "Q17391638"
         }
      ]
   }
}
#+END_SRC

** add occupation information

Modifying ~json2json.py~ to add the Wikidata data for each found writer.

- Which occupation property to add? First? All? Only the writer-ones?
  - currently all occupations that are subclasses of writer are added

** test enrichment

#+BEGIN_SRC 
  ./json2json.py -n -w gndwriter.json DNBTitel.json.gz | grep "poet lawyer" > poetlawyer_gndwriter.json
#+END_SRC

#+BEGIN_SRC sh :results raw
  grep Egmont poetlawyer_gndwriter.json__ | head -n1 | json_pp
#+END_SRC

#+BEGIN_SRC json
{
   "contributor" : [
      "116924373"
   ],
   "_id" : "361432887",
   "place_publisher" : "München ; Leipzig : G. Müller",
   "title" : "Goethes Egmont in Schillers Bearbeitung",
   "lang" : "ger",
   "creator" : [
      "118540238"
   ],
   "issued" : "1914",
   "issued_norm" : 1914,
   "type" : "Document",
   "creator_wd" : {
      "118540238" : {
         "occupations" : [
            {
               "name" : "art critic",
               "id" : "Q4164507"
            },
            {
               "name" : "travel writer",
               "id" : "Q3579035"
            },
            {
               "name" : "poet",
               "id" : "Q49757"
            },
            {
               "name" : "poet lawyer",
               "id" : "Q1209498"
            },
            {
               "id" : "Q1350157",
               "name" : "music critic"
            },
            {
               "name" : "novelist",
               "id" : "Q6625963"
            },
            {
               "name" : "autobiographer",
               "id" : "Q18814623"
            },
            {
               "id" : "Q214917",
               "name" : "playwright"
            },
            {
               "id" : "Q3606216",
               "name" : "aphorist"
            },
            {
               "name" : "diarist",
               "id" : "Q18939491"
            },
            {
               "name" : "theologian",
               "id" : "Q1234713"
            },
            {
               "name" : "art theorist",
               "id" : "Q17391638"
            }
         ],
         "id" : "Q5879",
         "name" : "Johann Wolfgang von Goethe"
      }
   },
   "pages" : [
      "153 S."
   ],
   "publisher" : "G. Müller",
   "place" : [
      "München",
      "Leipzig"
   ],
   "medium" : "RDACarrierType/1044",
   "pages_norm" : 153
}
#+END_SRC


* TODO index in Elastic

- check what happens with JSON like this: "publisher":
  "Akad. Kiado\u0301" - is the [[http://www.fileformat.info/info/unicode/char/0301/index.htm][COMBINING ACUTE ACCENT]] correctly
  processed? similar: "publisher": "Museum fu\u0308r Tierkunde"
Queries:
- Median, Mean, etc. in Elastic? - [[https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-aggregation.html][percentiles]]
- location

** create index

| field             | type    | analysed | note                                           |
|-------------------+---------+----------+------------------------------------------------|
| ~_id~             | string  | no       | DNB ID                                         |
| ~contributor~     | string  |          |                                                |
| ~creator~         | string  |          |                                                |
| ~extent~          | string  |          | field is missing! *TODO: difference to pages?* |
| ~issued~          | string  |          |                                                |
| ~issued_norm~     | integer | no       | year                                           |
| ~lang~            | string  | no       | 3-letter code or empty                         |
| ~medium~          | string  | no       |                                                |
| ~pages~           | string  | no       |                                                |
| ~pages_norm~      | integer | no       |                                                |
| ~place~           | string  |          |                                                |
| ~place_publisher~ | string  |          |                                                |
| ~price~           | string  |          |                                                |
| ~publisher~       | string  |          |                                                |
| ~short_title~     | string  |          |                                                |
| ~subject~         | string  |          |                                                |
| ~title~           | string  | yes      |                                                |
| ~type~            | string  | no       |                                                |

** fill index
